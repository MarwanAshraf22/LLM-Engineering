{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1b72cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marwan/miniconda_3/envs/llm-engineering-3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display,Markdown,update_display\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d598a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "model = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917e6473",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = 'You are a helpful assistant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message,history):\n",
    "    messages = [{'role':'system','content':system_message}]\n",
    "    for user_message, assistant_message in history:\n",
    "        messages.append({'role':'user','content':user_message})\n",
    "        messages.append({'role':'assistant','content':assistant_message})\n",
    "    messages.append({'role':'user','content':message})\n",
    "\n",
    "    print('History is :')\n",
    "    print(history)\n",
    "    print('And message is:')\n",
    "    print(message)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=model, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a12fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marwan/miniconda_3/envs/llm-engineering-3.12/lib/python3.12/site-packages/gradio/chat_interface.py:338: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is :\n",
      "[]\n",
      "And message is:\n",
      "hi\n",
      "History is :\n",
      "[]\n",
      "And message is:\n",
      "Hi\n",
      "History is :\n",
      "[['Hi', 'Hello! How can I assist you today?']]\n",
      "And message is:\n",
      "do you know me\n",
      "History is :\n",
      "[['Hi', 'Hello! How can I assist you today?'], ['do you know me', \"I don’t have personal knowledge or memory of individual users. However, I’m here to help you with any questions or topics you'd like to discuss! What’s on your mind?\"]]\n",
      "And message is:\n",
      "I am marwan ashraf\n",
      "History is :\n",
      "[['Hi', 'Hello! How can I assist you today?'], ['do you know me', \"I don’t have personal knowledge or memory of individual users. However, I’m here to help you with any questions or topics you'd like to discuss! What’s on your mind?\"], ['I am marwan ashraf', 'Nice to meet you, Marwan Ashraf! How can I assist you today?']]\n",
      "And message is:\n",
      "what was the first question i asked you\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae89e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message,history):\n",
    "    messages = [{\"role\":\"system\",\"content\":system_message}] + history + [{\"role\":\"user\",\"content\":message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=model, messages=messages, stream=True)\n",
    "\n",
    "    response = ''   \n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66d8ff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat,type='messages').launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069bc47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
